"""
LangSmithë¥¼ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘ ì—ì´ì „íŠ¸ ìë™ í‰ê°€ ì˜ˆì‹œ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” LangSmith ë°ì´í„°ì…‹ì— ë“±ë¡ëœ ì¿¼ë¦¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬
ìŠ¤í¬ë˜í•‘/í¬ë¡¤ë§ ì—ì´ì „íŠ¸ë¥¼ ìë™ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

ì´ íŒŒì¼ì€ í‰ê°€ ì‹¤í–‰ì—ë§Œ ì§‘ì¤‘í•˜ë©°, ë°ì´í„°ì…‹ ìƒì„±/ìˆ˜ì •ì€
dataset_manager.pyì—ì„œ ë³„ë„ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•:
1. examples/create_scraping_datasets.pyë¡œ ì‡¼í•‘ ë°ì´í„°ì…‹ ìƒì„±
2. ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ìë™ í‰ê°€ ìˆ˜í–‰
3. LangSmithì—ì„œ í‰ê°€ ê²°ê³¼ í™•ì¸

ë°ì´í„°ì…‹ í˜•ì‹:
- inputs: {"query": "ê²€ìƒ‰ ì§ˆë¬¸"}
- outputs: {"expected_info": "ê¸°ëŒ€í•˜ëŠ” ì •ë³´ ìœ í˜•", "validation_criteria": ["ê²€ì¦ ê¸°ì¤€ë“¤"]}
"""

import asyncio
import os
import sys
import time
from datetime import datetime
from typing import Dict, Any
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from langsmith.evaluation import evaluate
from langgraph_sdk import get_client as get_langgraph_client
from src.langsmith_scape_eval.scrape_evaluators import get_scraping_evaluators
from src.langsmith_scape_eval.dataset_manager import DatasetManager
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()


async def run_scraping_agent(inputs: Dict[str, Any]) -> Dict[str, Any]:
    """
    ìŠ¤í¬ë˜í•‘ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    LangSmith í‰ê°€ì—ì„œ ì‚¬ìš©í•  ì‹¤ì œ ì—ì´ì „íŠ¸ ì‹¤í–‰ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” inputsë¥¼ ë°›ì•„ì„œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        inputs: {"query": "ì‚¬ìš©ì ì§ˆë¬¸"}
        
    Returns:
        Dict containing:
            - final_answer: ì—ì´ì „íŠ¸ì˜ ìµœì¢… ë‹µë³€
            - messages: ì›ë³¸ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (ë„êµ¬ í˜¸ì¶œ í¬í•¨)
            - execution_time: ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    """
    try:
        # Extract query from inputs
        query = inputs.get("query", "")
        if not query:
            return {
                "final_answer": "ì§ˆë¬¸ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "messages": [],
                "execution_time": 0
            }
        
        # Initialize LangGraph agent client
        # Make sure your LangGraph agent is running on this URL
        base_url = "http://127.0.0.1:2024"
        client = get_langgraph_client(url=base_url)
        
        # Create a new thread for this conversation
        thread = await client.threads.create()
        start_time = time.time()
        
        # Prepare input data for the agent
        # This includes system message and user query
        input_data = {
            "messages": [
                {
                    "role": "system", 
                    "content": "ë„ˆëŠ” ë¬´ì‹ ì‚¬ ê²€ìƒ‰ ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ìì— ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ë¬¼ê±´ì„ ê²€ìƒ‰í•´ì¤˜. ì •ë³´ê°€ ë¶€ì¡±í•˜ë”ë¼ë„ ìµœëŒ€í•œ ì˜ ì°¾ì•„ì•¼í•˜ë©° ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”êµ¬í•˜ë©´ ì•ˆë¼."
                },
                {
                    "role": "user", 
                    "content": query
                }
            ]
        }
        
        # Initialize result storage variables
        final_answer = ""
        messages = []
        
        # Execute agent and process streaming results
        async for chunk in client.runs.stream(
            thread["thread_id"],
            "agent",  # This should match your LangGraph agent name
            input=input_data,
            stream_mode="updates"
        ):
            if chunk.event == "updates":
                # Process agent messages
                if "agent" in chunk.data:
                    agent_msgs = chunk.data.get("agent", {}).get("messages", [])
                    for msg in agent_msgs:
                        messages.append(msg)
                        # Store the last message with content as final answer
                        if msg.get("content"):
                            final_answer = msg.get("content", "")
                
                # Process tool execution messages
                if "tools" in chunk.data:
                    tool_messages = chunk.data.get("tools", {}).get("messages", [])
                    for tool_msg in tool_messages:
                        messages.append(tool_msg)
        
        # Calculate execution time
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Debug output (can be removed in production)
        print(f"Query: {query}")
        print(f"Final answer length: {len(final_answer)} characters")
        print(f"Total messages: {len(messages)}")
        print(f"Execution time: {execution_time:.2f} seconds")
        
        return {
            "final_answer": final_answer,
            "messages": messages,
            "execution_time": execution_time
        }
        
    except Exception as e:
        # Handle any errors during agent execution
        error_message = f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"Error: {error_message}")
        return {
            "final_answer": error_message,
            "messages": [],
            "execution_time": 0
        }


def load_dataset(dataset_name: str) -> str:
    """
    DatasetManagerë¥¼ í™œìš©í•˜ì—¬ ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        dataset_name: ë¡œë“œí•  ë°ì´í„°ì…‹ ì´ë¦„
        
    Returns:
        str: ë°ì´í„°ì…‹ì˜ ID
        
    Raises:
        Exception: ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
    """
    try:
        manager = DatasetManager()
        
        # Get dataset info using DatasetManager
        info = manager.get_dataset_info(dataset_name)
        
        if "error" in info:
            raise Exception(info["error"])
        
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: {dataset_name} (ID: {info['id']})")
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
        print(f"   - ì´ë¦„: {info['name']}")
        print(f"   - ì„¤ëª…: {info['description']}")
        print(f"   - ì˜ˆì œ ìˆ˜: {info['example_count']}")
        print(f"   - ìƒì„±ì¼: {info['created_at']}")
        
        if info.get('sample_examples'):
            print(f"ğŸ“ ì²« ë²ˆì§¸ ì˜ˆì œ:")
            first_example = info['sample_examples'][0]
            print(f"   - Query: {first_example['inputs'].get('query', 'N/A')}")
            print(f"   - Expected Info: {first_example['outputs'].get('expected_info', 'N/A')}")
        
        return info['id']
        
    except Exception as e:
        error_msg = f"""
âŒ ë°ì´í„°ì…‹ '{dataset_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë¨¼ì € ë°ì´í„°ì…‹ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

1. ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ìƒì„±:
   python examples/create_scraping_datasets.py

2. ë˜ëŠ” ë°ì´í„°ì…‹ ê´€ë¦¬ì ì‹¤í–‰:
   python src/langsmith_scape_eval/dataset_manager.py

ìƒì„¸ ì˜¤ë¥˜: {str(e)}
        """
        print(error_msg)
        raise Exception(f"Dataset '{dataset_name}' not found. Please create it first.")


async def main():
    """
    ë©”ì¸ í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜
    
    ì´ í•¨ìˆ˜ëŠ” í‰ê°€ ì‹¤í–‰ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤:
    1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • í™•ì¸
    2. DatasetManagerë¡œ ê¸°ì¡´ ë°ì´í„°ì…‹ ë¡œë“œ
    3. ìŠ¤í¬ë˜í•‘ ì—ì´ì „íŠ¸ í‰ê°€ ì‹¤í–‰
    4. ê²°ê³¼ ì¶œë ¥ ë° ìš”ì•½
    """
    print("ğŸš€ LangSmith ìŠ¤í¬ë˜í•‘ ì—ì´ì „íŠ¸ í‰ê°€ ì‹œì‘")
    print("=" * 50)
    
    # Step 1: Environment validation
    langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
    openai_api_key = os.getenv("OPENAI_API_KEY")
    
    if not langsmith_api_key:
        print("âŒ ì˜¤ë¥˜: LANGSMITH_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— LANGSMITH_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    if not openai_api_key:
        print("âš ï¸  ê²½ê³ : OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   LLM ê¸°ë°˜ í‰ê°€ìê°€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # Step 2: Load dataset (do not create)
    dataset_name = "shopping_agent_dataset"  # Default dataset name (updated naming)
    try:
        dataset_id = load_dataset(dataset_name)
    except Exception as e:
        print(f"ğŸ’¥ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # Step 3: Get evaluators
    evaluators = get_scraping_evaluators()
    print(f"ğŸ” í‰ê°€ì ë¡œë“œ ì™„ë£Œ: {len(evaluators)}ê°œ")
    
    # Step 4: Set up evaluation
    experiment_name = f"scraping_agent_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    print(f"\nğŸ“Š í‰ê°€ ì„¤ì •:")
    print(f"   - ì‹¤í—˜ëª…: {experiment_name}")
    print(f"   - ë°ì´í„°ì…‹: {dataset_name}")
    print(f"   - í‰ê°€ì ìˆ˜: {len(evaluators)}")
    print(f"   - ë™ì‹œ ì‹¤í–‰ ìˆ˜: 1 (ì—ì´ì „íŠ¸ ë¶€í•˜ ê³ ë ¤)")
    
    # Step 5: Execute evaluation
    print(f"\nâ³ í‰ê°€ ì‹¤í–‰ ì¤‘...")
    try:
        # Wrap async function for LangSmith evaluate
        def sync_run_scraping_agent(inputs):
            """Synchronous wrapper for the async agent function."""
            return asyncio.run(run_scraping_agent(inputs))
        
        # Run LangSmith evaluation
        results = evaluate(
            sync_run_scraping_agent,      # Function to evaluate
            data=dataset_name,            # Dataset name or ID
            evaluators=evaluators,        # List of evaluators
            experiment_prefix=experiment_name,  # Experiment name
            max_concurrency=1,            # Concurrent executions (conservative for agent load)
            description="ìŠ¤í¬ë˜í•‘/í¬ë¡¤ë§ ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€"
        )
        
        # Step 6: Display results
        print(f"\nğŸ‰ í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“‹ ì‹¤í—˜ëª…: {experiment_name}")
        print(f"ğŸ”— ê²°ê³¼ í™•ì¸: https://smith.langchain.com/")
        
        # Display result summary if available
        if hasattr(results, 'results'):
            total_runs = len(results.results)
            print(f"ğŸ“Š ì´ í‰ê°€ ì‹¤í–‰ ìˆ˜: {total_runs}")
            
            # Calculate average scores across all evaluators
            all_scores = []
            for result in results.results:
                if hasattr(result, 'evaluation_results'):
                    total_score = sum(eval_result.score for eval_result in result.evaluation_results)
                    all_scores.append(total_score)
            
            if all_scores:
                avg_score = sum(all_scores) / len(all_scores)
                print(f"ğŸ“ˆ í‰ê·  ì´ì : {avg_score:.1f}/400 (4ê°œ í‰ê°€ì í•©ê³„)")
        
        print(f"\nğŸ’¡ íŒ: LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ ìƒì„¸í•œ í‰ê°€ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
        
    except Exception as e:
        print(f"ğŸ’¥ í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        print(f"\nğŸ”§ ë¬¸ì œ í•´ê²° ì²´í¬ë¦¬ìŠ¤íŠ¸:")
        print(f"   1. LangGraph ì—ì´ì „íŠ¸ê°€ http://127.0.0.1:2024ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸")
        print(f"   2. ì—ì´ì „íŠ¸ ì´ë¦„ì´ 'agent'ë¡œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        print(f"   3. í™˜ê²½ ë³€ìˆ˜ (LANGSMITH_API_KEY, OPENAI_API_KEY)ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        print(f"   4. ë°ì´í„°ì…‹ì´ ì˜¬ë°”ë¥´ê²Œ ìƒì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")


def run_single_evaluation():
    """
    ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ í‰ê°€ í…ŒìŠ¤íŠ¸
    
    ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€ ì „ì— ë‹¨ì¼ ì¿¼ë¦¬ë¡œ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ì—ì´ì „íŠ¸ê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•˜ëŠ” ìš©ë„ì…ë‹ˆë‹¤.
    """
    
    async def test_single():
        print("ğŸ§ª ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 30)
        
        # Test query
        test_input = {"query": "ë‚¨ì ì…”ì¸  ì¶”ì²œ"}
        
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_input['query']}")
        
        # Execute agent
        result = await run_scraping_agent(test_input)
        
        # Display results
        print(f"\nğŸ“Š ì‹¤í–‰ ê²°ê³¼:")
        print(f"   - ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")
        print(f"   - ìµœì¢… ë‹µë³€ ê¸¸ì´: {len(result['final_answer'])} ë¬¸ì")
        print(f"   - ë©”ì‹œì§€ ìˆ˜: {len(result['messages'])}")
        print(f"   - ìµœì¢… ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:")
        print(f"     {result['final_answer'][:200]}{'...' if len(result['final_answer']) > 200 else ''}")
        
        return result
    
    return asyncio.run(test_single())


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # Single query test mode
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘...")
        run_single_evaluation()
    else:
        # Full evaluation mode
        print("ğŸš€ ì „ì²´ í‰ê°€ ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘...")
        asyncio.run(main())